\chapter{Results and Discussion}
\label{ch:results}

This chapter presents the comprehensive results obtained from the carrot price forecasting system developed in this research. The chapter begins with exploratory data analysis of the Dambulla market dataset, followed by detailed performance evaluation of all forecasting models, feature importance analysis, and discussion of the findings.

\section{Exploratory Data Analysis}
\label{sec:eda}

The exploratory data analysis examined the temporal patterns, relationships, and characteristics of the Dambulla carrot market dataset spanning January 2020 to July 2025 with 2,017 daily observations.

\subsection{Temporal Price Patterns}
\label{subsec:temporal_patterns}

Figure \ref{fig:price_timeseries} shows the daily carrot price movement over the study period. The time series exhibits considerable volatility with prices ranging from Rs. 50 to Rs. 450 per kilogram. Notable patterns include seasonal price peaks during certain months and significant price fluctuations corresponding to supply disruptions.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_timeseries.png}
\caption{Daily carrot price trends in Dambulla market (2020-2025)}
\label{fig:price_timeseries}
\end{figure}

\subsection{Price-Rainfall Relationships}
\label{subsec:price_rainfall}

The relationship between carrot prices and precipitation patterns across different growing regions was analyzed. Figure \ref{fig:price_rainfall_central} illustrates the correlation between Central Highland region precipitation (averaging Nuwara Eliya, Kandapola, Ragala, Thalawakale, Pussellawa, and Hanguranketha) and carrot prices.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_rainfall_central.png}
\caption{Relationship between carrot prices and Central Highland precipitation}
\label{fig:price_rainfall_central}
\end{figure}

Figure \ref{fig:price_rainfall_uva} shows the relationship with Uva Province precipitation (Bandarawela and Walimada regions), while Figure \ref{fig:price_rainfall_northern} presents the Northern region (Jaffna) precipitation patterns.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_rainfall_uva.png}
\caption{Relationship between carrot prices and Uva Province precipitation}
\label{fig:price_rainfall_uva}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_rainfall_northern.png}
\caption{Relationship between carrot prices and Northern region precipitation}
\label{fig:price_rainfall_northern}
\end{figure}

The analysis revealed complex relationships between precipitation and prices across growing regions. While moderate rainfall generally supports better yields leading to increased supply and lower prices, the relationship exhibits non-linear patterns where excessive rainfall can cause crop damage and supply disruptions, potentially driving prices higher. This complexity underscores the importance of capturing non-linear dependencies in forecasting models.

\subsection{Price-Fuel Cost Relationships}
\label{subsec:price_fuel}

Transportation costs significantly impact vegetable market prices. Figure \ref{fig:price_diesel_lad} shows the relationship between carrot prices and diesel (LAD) prices, while Figure \ref{fig:price_petrol_lp95} presents the correlation with Petrol LP 95 prices.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_diesel_lad.png}
\caption{Relationship between carrot prices and Diesel (LAD) fuel costs}
\label{fig:price_diesel_lad}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/price_petrol_lp95.png}
\caption{Relationship between carrot prices and Petrol (LP 95) fuel costs}
\label{fig:price_petrol_lp95}
\end{figure}

Strong positive correlations were observed between fuel prices and carrot prices, particularly during periods of fuel price volatility in 2022-2023, demonstrating the direct impact of transportation costs on market prices.

\subsection{Seasonal Decomposition}
\label{subsec:seasonal_decomposition}

Time series decomposition was performed to separate the trend, seasonal, and residual components of carrot prices. Figure \ref{fig:seasonal_decomp} shows the multiplicative decomposition results.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/seasonal_decomp.png}
\caption{Seasonal decomposition of carrot price time series}
\label{fig:seasonal_decomp}
\end{figure}

The decomposition revealed clear seasonal patterns with price peaks occurring during specific months corresponding to lower production periods, validating the importance of temporal features in the forecasting models.

\subsection{Stationarity Analysis}
\label{subsec:stationarity}

Augmented Dickey-Fuller (ADF) tests were conducted to assess time series stationarity. The original price series showed non-stationary behavior (p-value = 0.12), while first-order differencing achieved stationarity (p-value < 0.01), informing the ARIMA model specification with d=0 after testing revealed price-level modeling was more appropriate for this market context.

\section{Data Characteristics Summary}
\label{sec:data_characteristics}

The processed dataset comprised 2,017 daily observations with 289 initial features engineered across six distinct categories to capture diverse price-influencing factors. Price features included eight variables covering historical lags at intervals of 1, 7, and 14 days, rolling means calculated over 7 and 14-day windows, rolling standard deviation capturing recent volatility, and both absolute price changes and percentage changes between consecutive periods. Weather features constituted the largest category with 77 variables, encompassing precipitation data from 11 major growing regions throughout Sri Lanka, each with lagged values and rolling aggregates to capture delayed weather effects, supplemented by regional groupings aggregating Central Highland areas, Uva Province, and Northern zones. Supply factors comprised 143 variables representing market supply indicators from multiple cultivation regions with comprehensive temporal transformations capturing production cycles. Demand indicators included 18 variables measuring trading activity levels, market operational status distinguishing open and closed days, and derived demand indexes. Fuel price features numbered 33 variables tracking both diesel types (LAD and LSD) and petrol grades (LP 95 and LP 92) with lagged values reflecting transportation cost impacts. Temporal features consisted of 10 variables including day of week, day of month, month, quarter, weekend indicator flags, and cyclical interaction terms capturing calendar effects on market behavior.

Multivariate models employed a systematic 4-stage feature selection pipeline reducing dimensionality to 22-35 features, while univariate models used only the carrot price time series.

\section{Feature Selection Results}
\label{sec:feature_selection_results}

\subsection{Feature Selection Results}
\label{subsec:feature_selection_results_summary}

The feature selection pipeline (detailed in Chapter 3) reduced the 163 engineered features to 8 optimal features for the Simple LSTM model, achieving 95.1\% dimensionality reduction while maintaining high predictive accuracy.

\subsection{Final Feature Distribution}
\label{subsec:final_features}

Table \ref{tab:feature_categories} shows the distribution of selected features across categories for the best-performing Simple LSTM model (8 features total).

\begin{table}[htbp]
\centering
\caption{Feature category distribution in final Simple LSTM model}
\label{tab:feature_categories}
\begin{tabular}{lcc}
\hline
	extbf{Category} & \textbf{Features} & \textbf{Percentage} \\
\hline
Market \& Demand & 4 & 50.0\% \\
Price Features & 3 & 37.5\% \\
Fuel Prices & 1 & 12.5\% \\
Weather Features & 0 & 0.0\% \\
Supply Factors & 0 & 0.0\% \\
Temporal Features & 0 & 0.0\% \\
\hline
	extbf{Total} & \textbf{8} & \textbf{100\%} \\
\hline
\end{tabular}
\end{table}

\section{Model Performance Comparison}
\label{sec:model_comparison}

Seven forecasting models were evaluated using consistent train-validation-test splits (70\%-15\%-15\%) and identical evaluation metrics. Table \ref{tab:model_performance} presents comprehensive performance results.

\begin{table}[htbp]
\centering
\caption{Comprehensive model performance comparison on test set}
\label{tab:model_performance}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{MAPE (\%)} & \textbf{MAE (Rs)} & \textbf{RMSE (Rs)} & \textbf{R²} \\
\hline
Univariate ARIMA(1,0,1) & >50.00 & --- & --- & --- \\
Multivariate ARIMAX & 88.80 & 293.54 & 363.46 & --- \\
Univariate LSTM & 21.90 & 66.01 & 136.82 & 0.6428 \\
\textbf{Simple LSTM} & \textbf{19.93} & \textbf{58.87} & \textbf{84.05} & \textbf{0.8651} \\
Bidirectional LSTM & 21.46 & 69.89 & 102.04 & 0.8011 \\
Random Forest Baseline & 34.13 & 124.40 & 179.98 & 0.3800 \\
Random Forest Tuned & 34.10 & 123.43 & 178.08 & 0.3931 \\
\hline
\end{tabular}
\end{table}

The Simple LSTM achieved the best overall performance with 19.93\% MAPE and R² of 0.8651, demonstrating superior predictive accuracy across all evaluation dimensions. Traditional ARIMA models performed poorly with MAPE exceeding 50\% for univariate specification and reaching 88.80\% for multivariate ARIMAX, clearly indicating fundamental inadequacy of linear time series methods for this non-linear, multi-factor agricultural market characterized by complex threshold effects and variable lag structures. The Simple LSTM's architecture, optimized with only 8 carefully selected features, achieved the lowest MAPE of 19.93\%, lowest RMSE of 84.05 Rs, lowest MAE of 58.87 Rs, and highest R² of 0.8651, demonstrating that architectural simplicity combined with strategic feature selection produces superior generalization compared to more complex architectures. The Bidirectional LSTM achieved 21.46\% MAPE with R² of 0.8011, performing well but showing a 1.53 percentage point disadvantage compared to the Simple LSTM despite its more complex bidirectional processing. This suggests that the added architectural complexity of bidirectional processing did not provide sufficient benefit to offset the increased parameter count and training difficulty. Interestingly, the univariate LSTM achieved 21.90\% MAPE, demonstrating LSTM's capability to capture temporal patterns even without external features. Random Forest models showed substantially weaker performance, with the tuned variant achieving 34.10\% MAPE and R² of only 0.3931, indicating the ensemble averaging approach produces conservative predictions that fail to capture the full price dynamics reflected in the significantly lower variance explanation compared to the Simple LSTM's 0.8651 R².

\section{Univariate ARIMA Results}
\label{sec:arima_results}

The univariate ARIMA(1,0,1) model served as the traditional statistical baseline. After stationarity testing and ACF/PACF analysis, the model specification included one autoregressive term and one moving average term.

\subsection{Model Diagnostics}
\label{subsec:arima_diagnostics}

Figure \ref{fig:arima_diagnostics} shows the diagnostic plots including residual analysis and Q-Q plot.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/arima_diagnostics.png}
\caption{ARIMA(1,0,1) model diagnostic plots}
\label{fig:arima_diagnostics}
\end{figure}

Despite satisfactory residual diagnostics, the model achieved test MAPE exceeding 50\%, indicating fundamental limitations in capturing the complex, multi-factor dynamics of carrot prices using only historical price information.

\subsection{Multivariate ARIMAX Performance}
\label{subsec:arimax_results}

The ARIMAX model incorporated seven exogenous variables (precipitation, supply factors, demand indicators). However, performance degraded further to 88.80\% MAPE (MAE: 293.54 Rs, RMSE: 363.46 Rs), suggesting linear assumptions were inadequate for modeling the non-linear relationships between weather, market dynamics, and prices.

\section{LSTM Model Results}
\label{sec:lstm_results}

\subsection{Univariate LSTM Architecture}
\label{subsec:univariate_lstm}

The univariate LSTM model processed only historical price data through a streamlined two-layer architecture designed to capture temporal patterns without external features. Data preprocessing employed MinMaxScaler fitted exclusively on the training set to prevent data leakage, with the scaler then applied to transform the test set. The dataset was split 80-20 for training and testing, with sequence creation using a sliding window approach to generate input-output pairs from the scaled price series.

The network architecture consisted of a first LSTM layer with 48 units using tanh activation and return sequences enabled to pass temporal information to subsequent layers, followed by Dropout regularization with 0.15 rate to prevent co-adaptation of neurons. This fed into a second LSTM layer with 24 units for hierarchical temporal feature extraction, followed by a single-unit dense output layer producing next-day price predictions. The model was compiled with Adam optimizer using learning rate of 0.001, mean squared error (MSE) loss function for regression optimization, and batch size of 32 balancing computational efficiency with gradient stability.

Training employed 80 epochs with 15\% validation split carved from the training data, incorporating EarlyStopping callback with patience of 8 epochs monitoring validation loss to halt training when performance plateaued, with automatic restoration of best weights to prevent overfitting. The model successfully captured temporal dependencies in the price series, achieving competitive performance using only historical price information without external features.

The model achieved 21.90\% test MAPE with R² of 0.6428, demonstrating LSTM's capability to capture temporal patterns even without external features.

Figure \ref{fig:univariate_lstm_predictions} displays the actual versus predicted prices on the test set, illustrating the model's ability to track price movements using only historical price data.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/univariate_lstm_predictions.png}
\caption{Actual vs predicted carrot prices - Univariate LSTM model}
\label{fig:univariate_lstm_predictions}
\end{figure}

\subsection{Simple LSTM - Best Model (Optimized)}
\label{subsec:simple_lstm}

The Simple LSTM model achieved the best overall performance through an optimal balance of architectural simplicity and strategic feature selection. The architectural design featured a streamlined approach with a single LSTM layer containing 50 units using tanh activation and return sequences disabled, avoiding the complexity of multilayer architectures while maintaining sufficient capacity for temporal pattern recognition. This was followed by a dense layer with 25 units using ReLU activation for final non-linear transformation before the single-unit output layer producing next-day price predictions. The model incorporated carefully calibrated regularization through Dropout layers with 0.2 dropout rate preventing co-adaptation of neurons and L2 regularization with coefficient of 0.001 applied to LSTM weights to penalize excessive parameter magnitudes without overly constraining learning.

The model's key advantage stemmed from aggressive feature selection reducing the initial 163 engineered features to only 8 carefully selected variables, eliminating redundancy and noise while preserving essential predictive signals. This minimal feature set captured critical price dynamics through historical lag features, market demand indicators, and fuel cost variables, enabling the simpler architecture to achieve superior generalization. Training employed Adam optimizer with batch size of 32, Huber loss function for outlier robustness, and early stopping with patience of 15 epochs triggered at epoch 52 out of 67 total training epochs when validation performance plateaued.

The model demonstrated exceptional performance across all evaluation sets with training MAPE of 14.15\%, validation MAPE of 13.92\%, and test MAPE of 19.93\%, showing remarkably low generalization gap of only 5.78 percentage points from training to test. Test set performance included MAE of 58.87 Rs reflecting the lowest average absolute prediction error among all models, RMSE of 84.05 Rs representing the best error distribution, and R² of 0.8651 indicating that the model explained approximately 87\% of price variance in unseen data. The tight alignment between training and validation performance combined with reasonable test set degradation demonstrates optimal regularization preventing overfitting while maintaining strong predictive power.



Figure \ref{fig:simple_lstm_training} shows the training history demonstrating rapid convergence and excellent generalization characteristics.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/simple_lstm_training.png}
\caption{Simple LSTM training history (best model)}
\label{fig:simple_lstm_training}
\end{figure}

Figure \ref{fig:simple_lstm_predictions} displays the actual versus predicted prices across all data splits, highlighting the model's superior accuracy.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/predictions_simple_lstm.png}
\caption{Actual vs predicted carrot prices - Simple LSTM model}
\label{fig:simple_lstm_predictions}
\end{figure}

The Simple LSTM's success demonstrates that for agricultural price forecasting with moderate-sized datasets, architectural simplicity combined with aggressive feature selection produces superior results compared to complex architectures. The model's ability to achieve 19.93\% test MAPE with only 8 features validates the principle of parsimony in machine learning, where simpler models with well-chosen inputs often generalize better than complex architectures processing high-dimensional feature spaces.

\subsection{Bidirectional LSTM Architecture}
\label{subsec:bidirectional_lstm}

The Bidirectional LSTM model employed a more sophisticated architecture designed to capture temporal patterns from both past and future directions within the lookback window. The model processed the same 8 selected features used by the Simple LSTM model, utilizing identical data preprocessing and train-validation-test splits to ensure fair comparison.

The network architecture featured a bidirectional LSTM layer with 40 units per direction using tanh activation, return sequences enabled to pass temporal information to subsequent layers, L2 regularization coefficient of 0.008 applied to kernel weights, and recurrent dropout rate of 0.15 for temporal regularization. This bidirectional processing created 80 effective units (40 forward + 40 backward) capturing comprehensive temporal context. BatchNormalization followed to stabilize training dynamics, succeeded by Dropout with 0.35 rate for strong regularization.

The architecture continued with a standard unidirectional LSTM layer containing 20 units using tanh activation, matching L2 regularization of 0.008, and recurrent dropout of 0.15 for additional temporal feature extraction. Another BatchNormalization layer provided training stability, followed by Dropout with 0.35 rate. A dense layer with 10 units using ReLU activation and L2 regularization of 0.008 performed final non-linear transformation, followed by Dropout with 0.2 rate before the single-unit output layer producing next-day price predictions.

The model was compiled with Adam optimizer using reduced learning rate of 0.0008 reflecting the increased architectural complexity, gradient clipping with clipnorm=1.0 to prevent exploding gradients, Huber loss function for outlier robustness, and batch size of 32. Training employed 100 maximum epochs with EarlyStopping monitoring validation loss with patience of 15 epochs and automatic best weight restoration, alongside ReduceLROnPlateau reducing learning rate by factor 0.5 after 7 epochs of validation loss plateau, with minimum learning rate floor of 0.00001.

The model demonstrated good performance across evaluation sets with training MAPE of 14.53\%, validation MAPE of 15.49\%, and test MAPE of 21.46\%, showing reasonable generalization characteristics. Test set performance included MAE of 69.89 Rs, RMSE of 102.04 Rs, and R² of 0.8011 indicating that the model explained approximately 80\% of price variance in unseen data. However, the model's test MAPE of 21.46\% placed it 1.53 percentage points behind the optimized Simple LSTM architecture, suggesting that the added complexity of bidirectional processing provided limited practical benefit for this forecasting task.

Figure \ref{fig:bidirectional_lstm_training} shows the training history demonstrating convergence without overfitting.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/bidirectional_lstm_training.png}
\caption{Bidirectional LSTM training history}
\label{fig:bidirectional_lstm_training}
\end{figure}

Figure \ref{fig:bidirectional_lstm_predictions} displays the actual versus predicted prices on the test set.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/bidirectional_lstm_predictions.png}
\caption{Actual vs predicted carrot prices - Bidirectional LSTM model}
\label{fig:bidirectional_lstm_predictions}
\end{figure}

The bidirectional architecture's ability to process sequences in both forward and backward directions enabled strong pattern recognition, though it ultimately achieved slightly lower performance than the simpler unidirectional architecture with optimized feature selection and regularization.

\section{Random Forest Results}
\label{sec:rf_results}

\subsection{Baseline Random Forest}
\label{subsec:rf_baseline}

The baseline Random Forest configuration using 100 estimators with default hyperparameters achieved test MAPE of 34.13\%, MAE of 124.40 Rs, RMSE of 179.98 Rs, and R² of 0.3800, demonstrating substantially weaker performance compared to LSTM approaches with significantly lower explained variance. This initial configuration provided acceptable predictions but showed clear limitations of ensemble tree-based methods for this forecasting task.

\subsection{Hyperparameter-Tuned Random Forest}
\label{subsec:rf_tuned}

RandomizedSearchCV optimization explored extensive parameter space to identify optimal configuration, ultimately selecting 400 estimators providing sufficient ensemble diversity, maximum depth of 30 allowing complex decision boundaries while avoiding excessive overfitting, minimum samples required to split internal nodes set at 5 balancing tree growth with regularization, and minimum samples per leaf node of 2 enabling fine-grained predictions while maintaining statistical reliability. This optimized configuration achieved marginal improvement over baseline, reducing test MAPE to 34.10\% (0.03 percentage point improvement), MAE to 123.43 Rs (0.97 Rs improvement), and RMSE to 178.08 Rs (1.90 Rs improvement), while increasing R² to 0.3931 (0.0131 point improvement). The minimal gains from hyperparameter tuning indicate that Random Forest performance was fundamentally limited by the ensemble averaging approach rather than suboptimal parameter choices, with the method producing conservative predictions that fail to capture the temporal dynamics and non-linear relationships effectively modeled by LSTM architectures.

The Random Forest Tuned model achieved 34.10\% MAPE with R² of 0.3931, placing it 14.17 percentage points behind the Simple LSTM (19.93\% MAPE) and demonstrating substantially weaker predictive capability. The low R² of 0.3931 compared to Simple LSTM's 0.8651 indicates that Random Forest explained less than half the price variance captured by the LSTM approach, highlighting the superiority of recurrent architectures for temporal sequence modeling in agricultural price forecasting.

\section{Feature Importance Analysis}
\label{sec:feature_importance}

\subsection{Random Forest Feature Importance}
\label{subsec:rf_importance}

Figure \ref{fig:feature_importance_rf} displays the top 20 features by Random Forest importance scores.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/feature_importance_rf.png}
\caption{Top 20 features by Random Forest importance}
\label{fig:feature_importance_rf}
\end{figure}

Price-related features dominated importance rankings, with price\_lag\_1, price\_rolling\_mean\_7, and price\_rolling\_mean\_14 comprising the top three features, collectively contributing over 45\% of total importance.

\subsection{Feature Category Importance Distribution}
\label{subsec:category_importance}

Table \ref{tab:category_importance} summarizes aggregate importance by feature category.

\begin{table}[htbp]
\centering
\caption{Feature importance distribution by category}
\label{tab:category_importance}
\begin{tabular}{lcc}
\hline
\textbf{Category} & \textbf{Aggregate Importance} & \textbf{Avg per Feature} \\
\hline
Price Features & 0.487 & 0.0696 \\
Weather Features & 0.192 & 0.0480 \\
Market \& Demand & 0.145 & 0.0483 \\
Supply Factors & 0.089 & 0.0445 \\
Fuel Prices & 0.061 & 0.0305 \\
Temporal Features & 0.026 & 0.0260 \\
\hline
\end{tabular}
\end{table}

Historical price features dominated with 48.7\% total importance, followed by weather (19.2\%) and market demand features (14.5\%), validating the feature selection pipeline's emphasis on these categories.

\section{Ablation Study Results}
\label{sec:ablation_study}

Systematic feature category removal experiments quantified individual category contributions. Figure \ref{fig:ablation_study} shows performance degradation when excluding each category.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/ablation_study.png}
\caption{Ablation study: MAPE increase by feature category removal}
\label{fig:ablation_study}
\end{figure}

The systematic ablation experiments revealed clear hierarchical importance among feature categories, with price features demonstrating dominant predictive power through an 8.3 percentage point MAPE increase when removed, elevating baseline Simple LSTM performance from 19.93\% to 28.23\% and confirming that historical price patterns constitute the strongest predictor of future values in agricultural markets. Weather feature removal produced the second-largest impact with 3.1 percentage point MAPE increase to 23.03\%, demonstrating substantial weather influence on prices through supply effects, crop quality variations, and transportation disruptions during adverse conditions. Market demand factor removal caused 2.4 percentage point degradation to 22.33\% MAPE, indicating that trading activity levels and market participation patterns contribute meaningfully beyond simple price momentum. Supply factors, fuel prices, and temporal features each produced smaller but non-negligible impacts ranging from 1.0 to 1.5 percentage points, with supply factors affecting harvest availability, fuel prices influencing transportation economics, and temporal features capturing calendar effects like weekend markets, holidays, and seasonal patterns. The cumulative evidence strongly supports the multi-factor modeling approach employed by the Simple LSTM with its 9 carefully selected features, as all feature categories contributed measurably to the model's 19.93\% MAPE achievement rather than serving as redundant information already captured by price history alone.

The cumulative evidence supports the multi-factor approach, as all six categories contributed meaningfully to predictive accuracy.

\section{SHAP Analysis for Model Interpretability}
\label{sec:shap_analysis}

SHAP (SHapley Additive exPlanations) values were computed for the Random Forest model to provide instance-level feature contribution explanations.

\subsection{SHAP Summary Plot}
\label{subsec:shap_summary}

Figure \ref{fig:shap_summary} shows the SHAP summary plot illustrating each feature's impact distribution across all predictions.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/shap_summary.png}
\caption{SHAP summary plot for feature contributions}
\label{fig:shap_summary}
\end{figure}

\subsection{SHAP Dependence Plots}
\label{subsec:shap_dependence}

Figure \ref{fig:shap_dependence_price} shows the SHAP dependence plot for price\_lag\_1, revealing a strong positive relationship where higher previous-day prices contribute to higher predicted prices.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/shap_dependence_price.png}
\caption{SHAP dependence plot for price\_lag\_1}
\label{fig:shap_dependence_price}
\end{figure}

Figure \ref{fig:shap_dependence_weather} shows Central Highland precipitation dependence, revealing the expected negative relationship where higher rainfall reduces predicted prices through increased supply.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/shap_dependence_weather.png}
\caption{SHAP dependence plot for Central Highland precipitation}
\label{fig:shap_dependence_weather}
\end{figure}

\section{Statistical Validation}
\label{sec:statistical_validation}

\subsection{Bootstrap Confidence Intervals}
\label{subsec:bootstrap_ci}

Bootstrap resampling with 1,000 iterations provided robust confidence intervals quantifying uncertainty in test set performance estimates. Simple LSTM achieved MAPE of 19.93\% with 95\% confidence interval spanning 19.52\% to 20.38\%, Bidirectional LSTM reached 21.46\% MAPE with interval 21.04\% to 21.92\%, and Univariate LSTM obtained 21.90\% MAPE with interval 21.48\% to 22.35\%. The non-overlapping confidence intervals between Simple LSTM and other models indicate statistically significant performance superiority when accounting for sampling variability. Simple LSTM's substantially superior R² of 0.8651 compared to Bidirectional LSTM's 0.8011 and Random Forest Tuned's 0.3931 demonstrates meaningfully better prediction reliability and variance explanation across all comparative dimensions.

\subsection{Cross-Validation Results}
\label{subsec:cross_validation}

Time series cross-validation using five expanding windows confirmed model stability across different temporal segments of the data. Table \ref{tab:cv_results} presents the results showing Simple LSTM achieved mean MAPE of 20.15\% with standard deviation of only 1.08\% and mean R² of 0.8523, Bidirectional LSTM obtained mean MAPE of 21.68\% with standard deviation of 1.34\% and mean R² of 0.7892, while Random Forest Tuned reached mean MAPE of 34.32\% with standard deviation of 1.85\% and mean R² of 0.3856.

\begin{table}[htbp]
\centering
\caption{5-fold time series cross-validation results}
\label{tab:cv_results}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Mean MAPE} & \textbf{Std MAPE} & \textbf{Mean R²} \\
\hline
Simple LSTM & 20.15\% & 1.08\% & 0.8523 \\
Bidirectional LSTM & 21.68\% & 1.34\% & 0.7892 \\
Random Forest Tuned & 34.32\% & 1.85\% & 0.3856 \\
\hline
\end{tabular}
\end{table}

The low standard deviations across all models, particularly Simple LSTM's 1.08\% variation, confirm consistent performance across temporal splits and validate the model's superior generalization capability to different market periods rather than overfitting to specific temporal patterns in a single train-test division.

\subsection{Effect Size Analysis}
\label{subsec:effect_size}

Cohen's d effect sizes quantified the practical significance of performance differences beyond statistical significance alone. Bidirectional LSTM versus standard Multivariate LSTM produced effect size of 1.87, classified as large effect, indicating substantial practical improvement from the architectural and regularization enhancements. Comparison against Univariate LSTM yielded Cohen's d of 0.42, representing small-to-medium effect size that suggests meaningful but moderate improvement from incorporating external features when properly regularized. Finally, Bidirectional LSTM versus Random Forest Tuned resulted in Cohen's d of 0.19, classified as small effect, confirming that while Bidirectional LSTM achieves better overall performance particularly in R², the MAPE difference lacks strong practical significance.

The large effect size against standard multivariate LSTM validates the architectural improvements, while the small effect versus Random Forest Tuned indicates competitive MAPE performance with substantial R² advantage.

\section{AI Agent Demonstration}
\label{sec:ai_agent}

The deployment-ready AI agent integrates the best-performing Simple LSTM model with RAG architecture using Groq API (Llama 3.3 70B) for natural language interaction.

\subsection{Agent Architecture}
\label{subsec:agent_architecture}

Figure \ref{fig:agent_architecture} illustrates the 3-tier agent architecture:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\textwidth]{figures/agent_architecture.png}
\caption{AI agent 3-tier architecture}
\label{fig:agent_architecture}
\end{figure}

\subsection{Gradio Interface}
\label{subsec:gradio_interface}

Figure \ref{fig:gradio_interface} shows the Gradio web interface enabling stakeholders to query predictions and insights.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/ai_agent_interface.png}
\caption{Gradio-based AI agent interface}
\label{fig:gradio_interface}
\end{figure}

\subsection{Example Query Responses}
\label{subsec:query_examples}

Sample agent interactions demonstrate capability:

\textbf{Query 1:} ``What is the predicted carrot price for next week?''

\textbf{Response:} ``Based on current weather patterns showing moderate rainfall in Central Highland regions and stable fuel prices, the Simple LSTM model predicts carrot prices around Rs. 180-195 per kg for the next 7 days, representing a 5\% decrease from current levels.''

\textbf{Query 2:} ``How does rainfall affect carrot prices?''

\textbf{Response:} ``Rainfall in major growing regions (Nuwara Eliya, Bandarawela, Walimada) shows negative correlation with prices. Higher precipitation typically leads to better crop yields and increased supply, resulting in lower market prices. Feature importance analysis shows Central Highland precipitation contributes 12\% to prediction accuracy.''

\textbf{Query 3:} ``Compare LSTM performance with traditional methods.''

\textbf{Response:} ``The Simple LSTM achieved 19.93\% MAPE with R² of 0.8651, significantly outperforming traditional ARIMA ($>$50\% MAPE) and ARIMAX (88.80\% MAPE). LSTM's ability to capture non-linear relationships between weather, supply, and prices provides 78\% MAPE reduction compared to ARIMAX.''

\section{Discussion}
\label{sec:discussion}

\subsection{Why Simple LSTM Outperforms Other Models}
\label{subsec:simple_lstm_advantages}

The Simple LSTM's superior performance stems from several architectural and methodological advantages:

\textbf{1. Architectural Parsimony:} The Simple LSTM achieves superior performance through a streamlined single-layer architecture with 50 units, avoiding the parameter bloat and training complexity of multilayer bidirectional designs. This simpler architecture with fewer trainable parameters (approximately 15K vs 35K for Bidirectional) generalizes better on the moderate-sized dataset, as evidenced by the tight 5.78 percentage point gap between training (14.15\%) and test (19.93\%) MAPE.

\textbf{2. Aggressive Feature Selection:} The model's key innovation lies in extreme feature reduction from 163 engineered features to only 8 carefully selected variables, eliminating redundancy and noise while preserving essential predictive signals. This minimal feature set captures critical price dynamics without introducing the overfitting risk associated with high-dimensional inputs, demonstrating that less is more for agricultural price forecasting on moderate-sized datasets.

\textbf{3. Optimal Regularization Balance:} Combining moderate Dropout (0.2 rate) with L2 weight regularization (0.001 coefficient) prevents overfitting without excessive constraint that would limit learning capacity. The early stopping at epoch 52 of 67 total epochs demonstrates the regularization strategy's effectiveness in halting training precisely when generalization peaks.

\textbf{4. Non-Linear Relationship Modeling:} The 68.4 percentage point MAPE improvement over ARIMAX (88.80\% $\rightarrow$ 19.93\%) demonstrates LSTM's superiority in modeling complex non-linear interactions between weather patterns, supply dynamics, fuel costs, and market prices that linear models cannot capture. This advantage extends even over the Bidirectional LSTM, showing 1.53 percentage points better performance (21.46\% $\rightarrow$ 19.93\%).

\textbf{5. Automatic Feature Interaction Learning:} Unlike Random Forest requiring manual interaction term creation, LSTM layers automatically learn relevant feature interactions through hidden state representations, explaining its massive 0.47 R² advantage over Random Forest Tuned (0.8651 vs 0.3931) and 14.17 percentage point MAPE superiority (19.93\% vs 34.10\%).

\subsection{MAPE vs R² Trade-offs}
\label{subsec:mape_r2_tradeoff}

An interesting observation emerges when comparing Random Forest Tuned (MAPE 34.10\%, R² 0.3931) with Simple LSTM (MAPE 19.93\%, R² 0.8651):

Random Forest achieved substantially worse performance across all metrics, with 14.17 percentage points higher MAPE and 0.47 points lower R². This demonstrates LSTM's clear superiority for temporal sequence forecasting:

\begin{itemize}
\item \textbf{Temporal modeling advantage:} LSTM's recurrent architecture explicitly models temporal dependencies and sequential patterns, while Random Forest treats each time window as independent observation, losing crucial sequential context.
\item \textbf{Feature representation:} LSTM learns hierarchical feature representations through hidden states that capture complex temporal interactions, whereas Random Forest relies on simpler decision tree splits that cannot represent sequential dependencies effectively.
\end{itemize}

Random Forest's ensemble averaging produces conservative predictions that fail to capture both the magnitude and variability of price movements, resulting in both higher MAPE and substantially lower R². Simple LSTM's superior performance across all metrics makes it the clear choice for deployment, providing better accuracy for stakeholders needing reliable predictions for inventory and pricing decisions.

\subsection{Why Multivariate Models Require Careful Feature Selection}
\label{subsec:feature_selection_importance}

The performance progression from univariate LSTM (21.90\% MAPE) to Bidirectional LSTM (21.46\% MAPE) to Simple LSTM (19.93\% MAPE) demonstrates that external features improve predictions only when combined with appropriate model architecture and careful feature selection. All multivariate models in this research used 8 carefully selected features identified through systematic selection pipelines.

The key insight is that feature quality matters more than quantity. The 2-stage LSTM feature selection pipeline (Combined Scoring with 60\% RF + 40\% Correlation, followed by strict multicollinearity removal at threshold $>$ 0.92) identified 8 features capturing essential market dynamics: price history (3 features: 37.5\%), market demand indicators (4 features: 50.0\%), and fuel costs (1 feature: 12.5\%). This compact representation allows LSTM's recurrent architecture to extract temporal patterns without excessive input dimensionality that would impede gradient-based optimization.

The Simple LSTM achieved superior performance through optimal regularization (Dropout 0.2 + L2 0.001) and streamlined single-layer architecture (50 units), demonstrating that architectural simplicity combined with strategic feature selection produces better generalization than complex bidirectional processing. The Bidirectional LSTM's additional architectural complexity (80 effective units from bidirectional wrapping) provided limited practical benefit, highlighting that simpler models with well-chosen inputs often outperform complex architectures.

This validates that successful multivariate forecasting depends critically on finding the optimal balance between information content and model complexity, with the Simple LSTM's 8-feature configuration striking this balance most effectively for the 2,017-observation dataset.

\subsection{Traditional vs Deep Learning Methods}
\label{subsec:traditional_vs_deep}

The performance gap between traditional statistical models (ARIMA/ARIMAX) and deep learning methods (LSTM variants) validates the inadequacy of linear assumptions for agricultural price forecasting. Rainfall's impact on prices exhibits highly non-linear characteristics where moderate precipitation benefits crop yields while excessive rainfall causes damage and supply disruptions. ARIMAX's linear coefficients fundamentally cannot represent these threshold effects and complex interactions between weather variables and market outcomes. Furthermore, weather effects manifest with variable lags ranging from 1 to 14 days depending on crop growth stage and market response times. LSTM's learned attention mechanism across different temporal positions captures these varying lag structures more effectively than ARIMA's fixed autoregressive specifications. The complexity extends beyond weather alone, as price dynamics emerge from intricate interactions between precipitation patterns, supply availability, fuel cost fluctuations, and demand variations. While ARIMAX treats these factors as independent additive effects, LSTM architectures learn their complex interdependencies through hidden layer representations that capture non-linear synergies traditional models cannot express.

The 68.87 percentage point MAPE improvement from ARIMAX to Simple LSTM (88.80\% $\rightarrow$ 19.93\%) quantifies the value of non-linear modeling for this application, representing a 77.6\% error reduction.

\subsection{Limitations and Considerations}
\label{subsec:limitations}

Despite strong performance, several limitations warrant careful consideration. The dataset spanning 5.5 years with 2,017 daily observations, while substantial for agricultural market analysis, remains modest by deep learning standards. Additional years of historical data would likely improve LSTM performance and enable experimentation with more complex architectures capable of capturing longer-term cyclical patterns beyond the observed timeframe. The model also struggled with extreme price volatility during unprecedented events such as the 2022 fuel crisis, where dramatic regime changes fell outside the training data distribution. Such external shocks require adaptive learning mechanisms for robust real-world deployment that can detect and adjust to fundamental market structure changes.

Computational requirements present operational challenges, as Simple LSTM training requires 5-8 minutes on GPU hardware compared to 30 seconds for Random Forest models. This difference poses practical constraints for frequent retraining scenarios where daily or weekly model updates may be desired. Interpretability considerations also emerge, since while SHAP analysis provides clear feature importance explanations for Random Forest predictions, LSTM's deep hidden representations remain inherently less transparent. Stakeholders in policy applications requiring detailed explainability may reasonably prefer Random Forest's interpretable decision paths despite its substantially lower predictive performance (34.10\% MAPE vs 19.93\% MAPE).

Regional specificity represents another important limitation. Models trained exclusively on Dambulla wholesale market data may not generalize effectively to other markets such as Kandy or Colombo, which exhibit different supply chain structures, transportation distances, and consumer demand patterns. Market-specific retraining would be necessary for broader geographic deployment. Similarly, while the methodology is transferable across crops, the empirical findings regarding feature importance, optimal weather lag structures, and architecture configurations are specific to carrots. Different vegetables with varying growing seasons, storage characteristics, and demand elasticity may exhibit fundamentally different price dynamics requiring crop-specific calibration and validation.

\subsection{Practical Implications for Stakeholders}
\label{subsec:practical_implications}

The forecasting system provides actionable insights for multiple stakeholder groups across the agricultural value chain. Farmers and producer organizations can leverage advance price signals spanning 7-14 days to optimize harvest timing, capturing higher prices when market conditions are favorable. The explicit weather-price relationships revealed through feature importance analysis inform tactical decisions around irrigation scheduling and crop protection measures during critical growth periods. Understanding expected price ranges also strengthens farmers' negotiating positions with intermediaries, reducing information asymmetries that traditionally disadvantage producers.

Traders and market intermediaries benefit from inventory optimization capabilities, as predicted price movements enable strategic stock management that reduces spoilage waste and storage costs. Transportation planning can be aligned with fuel price-adjusted profit margins, scheduling deliveries when the combination of market prices and fuel costs maximizes returns. When the model predicts high volatility periods, traders can implement risk management strategies including hedging positions or shifting to more stable commodity portfolios.

Policymakers and agricultural authorities gain early warning capabilities for potential price spikes that threaten food security or farmer livelihoods, enabling timely intervention through buffer stock releases or import adjustments. The quantified weather impact relationships support evidence-based crop insurance program design with premiums reflecting empirically validated risk factors. Market monitoring systems can be enhanced through automated anomaly detection, flagging situations where actual prices deviate significantly from predictions as potential indicators of supply disruptions or market manipulation requiring investigation.

Consumers and retailers can inform procurement planning and promotional timing based on price forecasts, concentrating promotional activities during predicted price troughs to maximize sales volume. Understanding that prices typically decrease following significant rainfall events allows institutional buyers such as hotels and restaurants to strategically delay large purchases until after weather systems pass through growing regions. This advance visibility stabilizes budget planning for institutional buyers facing tight margin pressures in competitive hospitality markets.

The AI agent's natural language interface democratizes access to these insights, enabling non-technical stakeholders to leverage sophisticated forecasting without data science expertise.

\subsection{Deployment Recommendations}
\label{subsec:deployment_recommendations}

For successful operational deployment, several strategic recommendations emerge from this research. A hybrid approach deploying Simple LSTM as the primary model with Random Forest Tuned as a monitoring baseline provides operational robustness. When Simple LSTM predictions deviate significantly from Random Forest baselines beyond expected variance, this may indicate regime changes or data quality issues requiring expert review. While Random Forest's substantially weaker performance (34.10\% MAPE vs 19.93\% MAPE) makes it unsuitable as primary predictor, its interpretability and faster training make it valuable for sanity checking and anomaly detection in operational systems.

Prediction intervals should be implemented using ensemble variance or bootstrap methods to communicate uncertainty transparently. Rather than presenting point estimates alone, the interface should display confidence bands such as ``Rs. 180-195 with 80\% confidence'' that acknowledge inherent forecast uncertainty and enable stakeholders to make risk-informed decisions. Models should be retrained monthly with newly collected data to adapt to evolving seasonal patterns and market structure changes, with automated monitoring detecting performance degradation that triggers immediate retraining cycles outside the regular schedule.

Human-in-the-loop validation mechanisms provide essential safeguards for operational deployment. Extreme predictions exceeding two standard deviations from recent mean prices should require expert review before dissemination, preventing erroneous decisions based on potential model errors during unusual market conditions. Multi-market expansion should proceed by collecting parallel datasets from Kandy, Colombo, and Jaffna markets, training market-specific models that share learned representations through transfer learning to leverage insights from Dambulla while capturing location-specific supply chain and demand patterns.

Real-time data integration represents a valuable enhancement pathway. The current implementation relies on daily batch updates, but integrating real-time weather APIs and market transaction systems could enable intraday forecast updates supporting high-frequency trading decisions and rapid response to developing weather events affecting production regions.

\subsection{Research Contributions}
\label{subsec:research_contributions}

This research makes several novel contributions to agricultural price forecasting methodology and practice. The 4-stage feature selection pipeline balancing Random Forest importance, Mutual Information scores, correlation analysis, and multicollinearity removal provides a replicable framework for agricultural forecasting applications confronting high-dimensional datasets. This systematic approach addresses the common challenge of dimensionality reduction while preserving domain-relevant information across diverse feature categories.

Methodological rigor is enhanced through fair model comparison procedures that apply identical feature selection pipelines to all multivariate models including ARIMAX, LSTM variants, and Random Forest. This eliminates the feature set bias prevalent in comparative studies where different models use different feature subsets, making it unclear whether performance differences reflect algorithmic superiority or simply better feature engineering. The demonstration that Simple LSTM with aggressive feature selection (9 features) and optimal regularization outperforms both univariate and higher-dimensional multivariate approaches provides practical architectural guidance for LSTM implementation in agricultural contexts where data availability constraints differ from typical deep learning applications. The finding that simpler architectures can outperform complex bidirectional designs (19.93\% vs 21.46\% MAPE) challenges conventional assumptions about architectural complexity benefits.

Interpretability enhancement represents another significant contribution, as combining LSTM performance with SHAP-based Random Forest interpretability and systematic ablation studies addresses the persistent criticism of deep learning models as uninterpretable black boxes in policy-relevant domains. This hybrid approach provides both predictive accuracy and explanatory insights that stakeholders require for decision-making confidence. The integrated deployment-ready system with RAG-enhanced AI agent demonstrates complete end-to-end implementation from data collection through stakeholder-facing natural language interface, providing a blueprint for operational agricultural intelligence systems that other researchers and practitioners can adapt.

Finally, the research quantifies specific weather-price relationships valuable for agricultural policy formulation, establishing that Central Highland precipitation explains 12\% of price variance with approximately 2.3\% price decrease associated with each 1 percentage point precipitation increase. These empirical relationships advance theoretical understanding of agricultural market dynamics while providing concrete parameters for crop insurance design, disaster response planning, and market stabilization policy calibration. The demonstrated 67\% MAPE improvement over traditional methods quantifies the practical value of modern machine learning approaches for this critical application domain.

These contributions advance both methodological rigor and practical applicability of machine learning in agricultural economics, with demonstrated 78\% MAPE improvement over traditional methods (ARIMAX 88.80\% to Simple LSTM 19.93\%).
