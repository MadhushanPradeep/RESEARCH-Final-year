\chapter{Design and Methodology}

This chapter presents the systematic approach employed to develop and evaluate machine learning models for forecasting wholesale carrot prices in the Dambulla market. The research methodology encompasses six major phases: systematic framework design, data collection and preparation, exploratory data analysis, feature engineering and selection, model development and training, model evaluation, and AI agent implementation. Each phase is designed to address specific research objectives while maintaining methodological rigor throughout the investigation.

\section{Systematic Framework}

The overall research framework follows a structured pipeline that progresses from raw data acquisition through model deployment. This systematic approach ensures reproducibility, methodological transparency, and practical applicability of the forecasting system.

The framework consists of several interconnected stages progressing systematically from raw data to deployed predictions. The process initiates with comprehensive data acquisition gathering information from multiple authoritative sources including market prices from the Central Bank of Sri Lanka, weather data from Copernicus covering eleven growing regions, fuel prices from Ceylon Petroleum Corporation, and supply-demand indicators from agricultural market reports. This raw data undergoes rigorous preprocessing involving quality assurance procedures, missing value treatment using forward-filling to maintain temporal integrity, outlier detection and analysis with deliberate retention of legitimate extreme values representing genuine market volatility, and necessary data transformations to prepare variables for modeling.

Following data preparation, exploratory data analysis employs visual and statistical techniques to understand temporal patterns in price movements, seasonal cycles, relationships between prices and external factors, and distributional characteristics informing modeling decisions. Feature engineering then creates meaningful predictors from raw variables, generating lag features to capture temporal dependencies, rolling statistics providing smoothed trend information, temporal features encoding calendar effects, and interaction terms representing synergistic relationships between variables. The engineered features undergo systematic selection using combined methods including Random Forest importance for non-linear relationship detection, Mutual Information for statistical dependency measurement, and Recursive Feature Elimination for iterative refinement, ultimately identifying optimal predictor subsets for multivariate models while univariate approaches utilize only historical prices.

Model development implements multiple forecasting approaches including traditional ARIMA for statistical baseline establishment, LSTM neural networks in univariate, standard multivariate, and bidirectional configurations for deep learning temporal pattern recognition, and Random Forest for ensemble learning perspective. Each developed model undergoes comprehensive evaluation using multiple performance metrics including MAPE, MAE, RMSE, and R² alongside statistical validation through bootstrap confidence intervals, cross-validation for generalization assessment, and ablation studies quantifying feature category contributions. The final stage integrates the best-performing model into a Retrieval-Augmented Generation system powered by Groq API, providing natural language query capabilities through Gradio web interface for accessible predictions supporting stakeholder decision-making.

This end-to-end methodology addresses the complete lifecycle of an agricultural price forecasting system, from conception through deployment, ensuring that each component receives appropriate attention while maintaining focus on delivering actionable price forecasts. Figure \ref{fig:methodology_flow} illustrates the complete methodological workflow showing the progression from data collection through AI agent deployment, highlighting the parallel feature engineering pipelines for Random Forest and LSTM models.

\begin{figure}[htbp]
\centering
\resizebox{!}{0.85\textheight}{\input{figures/methodology_flowchart.tex}}
\caption{Comprehensive Methodology Flow Diagram showing the systematic progression from data collection through model training and AI agent deployment. The diagram illustrates parallel feature engineering pipelines (RF: 273→22 features via 4-stage selection; LSTM: 163→8 features via 2-stage selection), evaluation across multiple model architectures, and final integration of the best-performing Simple LSTM model into an intelligent conversational AI agent.}
\label{fig:methodology_flow}
\end{figure}

\section{Data Collection and Preparation}

\subsection{Data Sources and Collection}

Historical data was systematically collected from four primary categories of sources to capture the complex factors influencing carrot prices.

\textbf{Market Price Data:} Daily wholesale carrot prices spanning January 2020 to July 2025 were obtained from the Central Bank of Sri Lanka database, which maintains comprehensive records of vegetable trading at the Dambulla Economic Centre. This dataset provides over 2,000 daily observations covering both normal market conditions and periods of economic volatility.

\textbf{Meteorological Data:} Precipitation measurements from eleven major carrot-growing regions across Sri Lanka (Nuwara Eliya, Kandapola, Ragala, Thalawakale, Pussellawa, Hanguranketha, Bandarawela, Walimada, Jaffna, and others) were sourced from the Copernicus Climate Data Store. Daily rainfall data enables capture of weather impacts on agricultural production and supply chains.

\textbf{Fuel Price Data:} Historical diesel and petrol prices were collected from the Ceylon Petroleum Corporation (ceypetco.gov.lk) official website. Transport fuel costs directly influence vegetable distribution costs and wholesale pricing in Sri Lanka's agricultural supply chain. Non-transport fuels (kerosene, furnace oils) were excluded as irrelevant to agricultural logistics.

\textbf{Market Indicators:} Supply factors from various cultivation regions, demand levels at Dambulla market, trading activity indicators, and market operational status (open/closed, holidays) were obtained from the Central Bank of Sri Lanka database. These variables capture market dynamics beyond simple price-weather relationships.

\subsection{Dataset Characteristics}

The final integrated dataset comprised 2,017 daily observations with 46 initial variables before feature engineering expanded the feature space. The temporal coverage spanned January 1, 2020 through July 11, 2025, capturing both stable market conditions and volatile periods including the 2022-2023 economic crisis affecting Sri Lanka. The target variable consisted of carrot prices measured in rupees per kilogram at Dambulla wholesale market, exhibiting significant fluctuations throughout the study period with prices ranging from minimum values near Rs. 50 during supply gluts to peaks exceeding Rs. 450 during scarcity periods, occasional spikes reaching even higher during extreme market stress. Missing values affected approximately 2\% of observations across variables, addressed through forward-filling methodology that propagates last observed values forward while avoiding introduction of future information that would create data leakage. Data quality remained high overall following systematic validation procedures and outlier treatment protocols, with the comprehensive temporal coverage providing sufficient observations for deep learning model training while including varied market conditions enhancing model generalization capability.

\subsection{Data Preprocessing}

Comprehensive preprocessing ensured data quality and prepared variables for modeling.

\textbf{Missing Value Treatment:} Forward filling (last observation carried forward) was employed for time series data. This method propagates the last observed value forward to fill subsequent missing entries, appropriate given that economic and meteorological variables typically change gradually. Forward filling was chosen over backward filling to avoid introducing future information into historical records, thereby preventing data leakage that could artificially inflate model performance.

\textbf{Supply Factor Transformation:} Supply factor variables originally encoded as (1=HIGH, 0=LOW, -1=NORMAL) were transformed to (2=HIGH, 1=NORMAL, 0=LOW) to create proper ordinal encoding suitable for machine learning algorithms. This transformation of 18 supply factor columns enables models to correctly interpret supply levels as ordered categories rather than arbitrary numeric codes.

\textbf{Fuel Column Filtering:} Non-transport fuel columns (kerosene, furnace oils) were systematically removed as they relate to industrial heating rather than agricultural transportation. Only transport-relevant fuels (petrol Lp\_95, Lp\_92; diesel lad, lsd) were retained, reducing dimensionality while maintaining predictive signal.

\textbf{Outlier Treatment:} Outlier analysis using the Interquartile Range (IQR) method identified 103 potential outliers (5.11\% of observations), with prices ranging up to Rs. 1,950 per kilogram. These extreme values were deliberately retained rather than removed or clipped, as they represent legitimate market volatility during supply shocks, seasonal festivals, and weather-driven scarcity events rather than measurement errors. This decision was supported by domain knowledge of Sri Lankan carrot markets, where genuine extreme price fluctuations occur during crisis periods.

\section{Exploratory Data Analysis}

Systematic exploratory analysis revealed patterns, relationships, and characteristics informing subsequent modeling decisions. This phase employed comprehensive visualization and statistical techniques using Python libraries (matplotlib, seaborn, pandas).

\subsection{Time Series Visualization}

\textbf{Historical Price Trends:} Time series plots of daily carrot prices revealed several patterns including seasonal cycles with higher prices during certain months, occasional sharp spikes corresponding to supply disruptions, and overall stability punctuated by periods of volatility. The 2022-2023 period showed increased volatility coinciding with Sri Lanka's economic crisis.

\textbf{Precipitation Patterns:} Regional rainfall visualization from major growing areas (particularly Nuwara Eliya) showed distinct seasonal patterns with heavy rainfall periods typically preceding supply disruptions and subsequent price increases. Lag effects between rainfall events and price movements were visually apparent.

\subsection{Correlation and Relationship Analysis}

\textbf{Correlation Heatmaps:} Comprehensive correlation analysis identified relationships between carrot prices and potential predictors. Price lag features showed strongest correlations (0.85-0.95), followed by rolling mean features (0.80-0.88). Among external factors, precipitation from central highland regions exhibited moderate negative correlations (-0.25 to -0.35), indicating that increased rainfall associates with lower prices, likely through improved supply.

\textbf{Scatter Plot Analysis:} Bivariate scatter plots between prices and key factors revealed non-linear relationships, particularly for precipitation (threshold effects where moderate rain supports production but excessive rain disrupts supply) and supply factors (categorical relationships rather than simple linear patterns).

\subsection{Distribution and Decomposition}

\textbf{Price Distribution:} Histograms and box plots revealed approximately normal distribution with slight positive skew (skewness = 1.89), with prices ranging from Rs. 25 to Rs. 1,950 per kilogram. Outlier analysis using Interquartile Range (IQR) method (Q1 - 1.5$\times$IQR, Q3 + 1.5$\times$IQR) identified 103 observations (5.11\% of total 2,017 observations) as potential outliers. These extreme values were deliberately retained as they represent legitimate market volatility during supply shocks, seasonal festivals, and adverse weather conditions rather than measurement errors or data quality issues. The retention of outliers was justified by domain knowledge indicating that carrot markets in Sri Lanka experience genuine extreme price fluctuations during crisis periods. The non-normal distribution necessitated the use of scaling methods robust to outliers, such as RobustScaler for LSTM preprocessing.

\textbf{Seasonal Decomposition:} Time series decomposition separated price data into trend, seasonal, and residual components. Analysis revealed gradual upward trend reflecting inflation, moderate seasonal patterns with peaks in certain months, and substantial residual volatility indicating strong influence of short-term factors beyond pure seasonality.

\section{Feature Engineering and Selection}

Feature engineering created meaningful predictors from raw variables, while systematic selection identified optimal subsets for different modeling approaches. Importantly, feature selection methodology differs between univariate and multivariate models, ensuring each approach receives appropriate input configuration.

\subsection{Feature Engineering}

Feature engineering was performed separately for Random Forest and LSTM models to accommodate their distinct architectural requirements. Random Forest, being a tree-based ensemble method, benefits from extensive feature spaces and explicit temporal encodings. In contrast, LSTM networks, with their recurrent architecture, require more focused feature sets to maintain efficient gradient propagation and avoid excessive dimensionality in sequential processing.

\textbf{Random Forest Feature Engineering:} Comprehensive engineering generated 273 derived variables across several categories:

\textbf{LSTM Feature Engineering:} Streamlined engineering created 163 focused features, emphasizing temporal patterns and market dynamics while maintaining computational efficiency for sequence processing.

For both approaches, feature engineering encompassed the following categories:

\textbf{Price Lag Features:} Seven lag variables (1, 2, 3, 7, 14, 21, 30 days) captured temporal dependencies in price movements. Additional features included first-order differences, percentage changes, and lag-differenced terms.

\textbf{Rolling Window Statistics:} Moving averages (7, 14, 30-day windows), standard deviations, minimum/maximum values, and medians provided smoothed trend information while capturing recent volatility patterns.

\textbf{Precipitation Features:} For each of eleven regions, lag features (1, 3, 7 days), rolling sums (7, 14 days), and regional group aggregations (central highlands, Uva province, northern, other) captured both immediate and delayed weather impacts.

\textbf{Supply and Fuel Features:} Lag features and rolling averages for supply factors and fuel prices captured delayed effects of production levels and transportation cost changes.

\textbf{Temporal Features:} Day of week, day of month, month, quarter, week of year, weekend indicators, month start/end flags, and cyclical encoding (sine/cosine transformations) captured calendar effects and seasonal patterns.

\textbf{Interaction Features:} Multiplicative terms between demand and trading activity, demand and market status, and market status and weekend captured synergistic effects between market variables.

\subsection{Model-Specific Feature Selection Strategies}

Feature selection methodologies were tailored to each model type, reflecting their distinct architectural characteristics and data processing requirements.

\subsubsection{Univariate Models (ARIMA, Univariate LSTM)}

\textbf{Feature Set:} Single feature---historical carrot prices only.

\textbf{Rationale:} Traditional univariate time series models assume future values depend solely on past observations of the same variable. No feature selection process required. These models serve as pure autoregressive baselines, establishing performance without external factors.

\textbf{ARIMA Configuration:} Parameters (p, d, q) determined through ACF/PACF analysis and AIC optimization on price series alone.

\textbf{Univariate LSTM Configuration:} 14-day lookback window of historical prices predicting next day's price. Architecture optimized for single-variable temporal patterns.

\subsubsection{Random Forest Feature Selection Pipeline}

Random Forest employed a comprehensive four-stage selection process optimized for tree-based ensemble learning:

\textbf{Stage 1 - Combined Scoring (60\% RF + 30\% MI + 10\% Correlation):} Three complementary metrics quantified feature importance. Random Forest importance (mean decrease in impurity, 100 estimators, depth 15) captured non-linear relationships and interactions. Mutual Information regression (5 neighbors) measured statistical dependencies including non-monotonic patterns. Pearson correlation identified linear associations. Scores normalized (0-1 range) and combined using weighted scheme: 0.60 $\times$ RF + 0.30 $\times$ MI + 0.10 $\times$ Correlation, emphasizing ensemble-based and information-theoretic criteria over simple linear correlation. Top 80 features by combined score advanced to subsequent stages.

\textbf{Stage 2 - Multicollinearity Removal:} Pairwise correlation matrix identified highly redundant features (correlation $\geq$ 0.95). From each correlated pair, feature with lower combined score removed, preserving predictive information while eliminating redundancy. This stage reduced 80 candidates to 47 features, removing 33 highly correlated variables.

\textbf{Stage 3 - SelectFromModel:} Random Forest-based SelectFromModel (300 estimators, median importance threshold) selected features exceeding ensemble's median importance, identifying 24 essential predictors.

\textbf{Stage 4 - Recursive Feature Elimination:} RFE (Random Forest, 200 estimators) iteratively removed least important features, independently identifying 24 optimal features. Final feature set comprised intersection of both methods (24 features), then refined by removing non-transport fuel features (kerosene lk\_lag\_1 and furnace oil fur\_1500\_high\_rolling\_mean\_7), yielding 22 transport-relevant features.

\textbf{Final Feature Set:} 22 features spanning six categories: weather features (54.5\%), supply features (22.7\%), price features (9.1\%), fuel features (4.5\% - diesel only), market features (4.5\%), temporal features (4.5\%). This distribution indicates weather and supply factors dominate carrot price dynamics beyond pure autoregressive patterns.

\subsubsection{LSTM Feature Selection Pipeline}

LSTM models employed a streamlined two-stage selection process optimized for sequential neural network architectures:

\textbf{Stage 1 - Combined Scoring with Priority System (60\% RF + 40\% Correlation):} Two complementary metrics quantified feature importance. Random Forest importance (100 estimators, depth 15) captured non-linear relationships. Pearson correlation identified linear associations. Scores normalized and combined: 0.60 $\times$ RF + 0.40 $\times$ Correlation. Mutual Information was excluded to maintain computational efficiency for iterative neural network training. Priority features (essential price lags, market indicators) were included regardless of score to ensure temporal continuity. Top 20 features by combined score plus priority features advanced to Stage 2.

	extbf{Stage 2 - Multicollinearity Removal:} Pairwise correlation matrix identified redundant features using a stricter threshold (correlation $>$ 0.92, slightly lower than Random Forest's 0.95 to account for LSTM's sensitivity to multicollinearity in gradient-based optimization). From each correlated pair, the feature with lower combined score was removed. This stage removed 11 features, yielding a final set of 8 features.

	extbf{Final Feature Set:} 8 features with balanced distribution: market features (50\%), price features (37.5\%), fuel features (12.5\%). This compact representation prioritizes direct market dynamics and recent price history, allowing LSTM's recurrent architecture to extract temporal patterns without excessive input dimensionality.

\section{Forecasting Model Development}

Five distinct modeling approaches were implemented to capture different aspects of temporal price dynamics.

\subsection{Train-Test Split Strategy}

Temporal split preserved chronological order ensuring no data leakage. The 2,017 observations were divided as follows: 70\% training (1,411 samples), 15\% validation (302 samples), 15\% testing (304 samples). For LSTM models, sequence creation with 14-day lookback window reduced the effective dataset size, yielding approximately 2,003 usable samples (1,402 training, 300 validation, 301 testing sequences). This approach prevents models from training on future observations, critical for valid time series forecasting evaluation.

\subsection{ARIMA Models}

AutoRegressive Integrated Moving Average models provided traditional statistical baseline.

\textbf{Univariate ARIMA:} Parameters (p, d, q) determined through Augmented Dickey-Fuller stationarity testing, Auto-Correlation Function (ACF), and Partial Auto-Correlation Function (PACF) analysis. Grid search over candidate values optimized Akaike Information Criterion (AIC). First-order differencing (d=1) achieved stationarity. Final model configuration selected based on lowest AIC while avoiding overfitting.

\textbf{Multivariate ARIMAX:} Extended ARIMA incorporating exogenous variables (precipitation, fuel, supply) alongside historical prices. Feature set matched LSTM multivariate configuration for fair comparison. ARIMAX enables assessment whether external factors improve traditional statistical forecasting.

\subsection{LSTM Models}

Long Short-Term Memory networks captured non-linear temporal dependencies through recurrent architecture. Two primary LSTM variants were implemented to explore different approaches to sequence modeling.

\textbf{Data Preparation:} Features scaled using RobustScaler (robust to outliers), target scaled separately. Sequences created with 14-day lookback window: each input comprises 14 consecutive days of features predicting next day's price. Sequence creation reduced effective dataset size from 2,017 to approximately 2,003 usable samples.

\textbf{Univariate LSTM:} Baseline LSTM trained only on historical price sequences. Architecture: Input layer (14 timesteps, 1 feature), LSTM layer (50 units, tanh activation, recurrent dropout 0.1), Batch Normalization, Dropout (0.3), Dense layer (25 units, relu), Dropout (0.2), Output (1 unit). Compiled with Adam optimizer (learning rate 0.001), Huber loss (robust to outliers), trained 100 epochs with batch size 32. Early stopping (patience 15) prevented overfitting.

	extbf{Multivariate LSTM (Simple Architecture):} Extended architecture processing 8 features simultaneously, selected through the two-stage LSTM feature selection pipeline. Architecture: Input (14 timesteps, 8 features), LSTM layer (50 units, tanh activation, L2 regularization 0.01, recurrent dropout 0.2), Batch Normalization, Dropout (0.4), Dense (10 units, relu, L2 regularization 0.01), Dropout (0.3), Output (1 unit). Optimized with Adam (learning rate 0.001, clipnorm 1.0), Huber loss, trained 100 epochs, batch size 32. Callbacks: Early Stopping (patience 15), ReduceLROnPlateau (factor 0.5, patience 7).

	extbf{Bidirectional LSTM:} Enhanced multivariate model with bidirectional processing, allowing network to learn from both forward and backward temporal context within sequences. Architecture: Input (14 timesteps, 8 features), Bidirectional LSTM wrapper (40 units per direction, tanh activation, L2 regularization 0.008, recurrent dropout 0.15), Batch Normalization, Dropout (0.35), LSTM (20 units, L2 regularization 0.008, recurrent dropout 0.15), Batch Normalization, Dropout (0.35), Dense (10 units, relu, L2 regularization 0.008), Dropout (0.2), Output (1 unit). Optimized with Adam (learning rate 0.0008, clipnorm 1.0), Huber loss, trained 100 epochs, batch size 32. Callbacks: Early Stopping (patience 15), ReduceLROnPlateau (factor 0.5, patience 7), ModelCheckpoint (save best). The bidirectional architecture effectively doubles the first layer capacity to 80 total units (40 forward + 40 backward), enabling richer temporal representations.

\subsection{Random Forest Regression}

Ensemble tree-based methods provided non-sequential baseline for comparison. Two Random Forest variants were implemented to establish baseline performance and explore optimization through hyperparameter tuning.

\textbf{Feature Representation:} Unlike LSTM's sequence input, Random Forest treats each day independently with lag features and rolling statistics providing temporal context. The 22 features selected through the four-stage pipeline (with domain-driven fuel refinement) were structured as single-row observations rather than sequences.

\textbf{Baseline Random Forest:} Initial configuration with default parameters provided starting point for optimization. Configuration: 100 estimators, maximum depth 15, minimum samples split 10, minimum samples leaf 5. This baseline established Random Forest's performance with standard hyperparameters before systematic tuning.

\textbf{Hyperparameter-Tuned Random Forest:} RandomizedSearchCV explored parameter space to identify optimal configuration. Search space: n\_estimators (100, 200, 300, 500), max\_depth (10, 15, 20, 25, 30, None), min\_samples\_split (2, 5, 10, 15), min\_samples\_leaf (1, 2, 4, 8), max\_features ('sqrt', 'log2', 0.5, 0.7), bootstrap (True, False). Search conducted with 50 iterations, 3-fold time series cross-validation, scoring on negative mean absolute error. Best parameters identified: n\_estimators=100, max\_depth=10, min\_samples\_split=2, min\_samples\_leaf=8, max\_features=0.7, bootstrap=True. These parameters were applied to final tuned model.

\section{Model Evaluation Framework}

Comprehensive evaluation ensured robust performance assessment and valid model comparison.

\subsection{Performance Metrics}

Four complementary metrics quantified forecasting accuracy:

\textbf{Mean Absolute Percentage Error (MAPE):} Scale-independent percentage error, intuitive interpretation as average prediction error percentage. Lower values indicate better performance. MAPE = $\frac{1}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right| \times 100\%$

\textbf{Mean Absolute Error (MAE):} Average absolute prediction error in original units (Rs), robust to outliers. MAE = $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$

\textbf{Root Mean Squared Error (RMSE):} Square root of average squared errors, penalizes large errors more heavily. RMSE = $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$

\textbf{R-squared (R²):} Proportion of variance explained, ranges 0-1 with higher indicating better fit. R² = $1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$

All metrics calculated on test set for unbiased performance estimation.

\subsection{Statistical Validation}

\textbf{Ablation Study:} Systematic removal of feature categories (weather, supply, fuel, price-only) quantified individual contributions. For each category removal, model retrained and performance degradation measured, revealing which factors most strongly influence predictions.

\textbf{Bootstrap Confidence Intervals:} 1000 bootstrap resamples from test set generated distribution of MAPE values, enabling 95\% confidence interval construction and statistical significance testing.

\textbf{Effect Size Analysis:} Cohen's d quantified magnitude of performance differences between models, distinguishing between statistically significant and practically meaningful improvements.

\textbf{SHAP Analysis:} SHapley Additive exPlanations computed feature importance for best model, providing model-agnostic interpretability through game-theoretic approach. SHAP values reveal both global feature importance and local prediction explanations.

\section{AI Agent Development Using Groq API}

The best-performing LSTM model was integrated into an intelligent conversational AI agent to provide accessible and interpretable price forecasts through a natural language interface. This implementation bridges the gap between complex machine learning predictions and practical stakeholder decision-making by enabling users to query price forecasts and obtain explanatory insights through intuitive conversational interactions rather than requiring technical expertise to interpret raw model outputs.

\subsection{System Architecture}

The AI agent operates through a three-tier architecture designed to process natural language queries, retrieve relevant prediction data, and generate contextually grounded responses. The system is powered by Groq's cloud-based large language model API, specifically leveraging the Llama 3.3 70B Versatile model for natural language understanding and generation capabilities.

The user interface layer implements a Gradio web framework providing an intuitive chat interface where stakeholders can pose questions in natural language. This responsive design ensures accessibility across desktop, tablet, and mobile devices, featuring real-time streaming responses that provide immediate feedback during query processing. The interface includes example queries to guide users, maintains conversation history for contextual follow-up questions, and generates public shareable links with 72-hour expiry to facilitate collaborative decision-making among agricultural stakeholders without requiring individual system installations.

The orchestration layer, implemented through the CarrotPriceAgent class, manages the core intelligence of the system by coordinating query processing and response generation. When a user submits a question, the query router analyzes the incoming text to extract temporal references using regex patterns for date identification in YYYY-MM-DD format, while simultaneously identifying keywords that trigger specific data retrieval pathways such as references to models, LSTM architectures, ARIMA comparisons, or forecast requests. The context builder then assembles relevant information by retrieving prediction records for date-specific queries, loading model performance metrics for comparison questions, incorporating data source documentation for methodological inquiries, and integrating domain knowledge about agricultural market dynamics compiled from research documentation. This structured context construction ensures that all language model responses are grounded in factual prediction data rather than relying on the language model's parametric knowledge, thereby minimizing the risk of hallucination and maintaining response accuracy.

The data layer provides three primary sources that serve as the factual foundation for all agent responses. The LSTM predictions CSV file contains approximately 2,000 test observations with detailed records including date, actual observed price, model-predicted price, prediction error, and MAPE for each observation, enabling precise answers to date-specific queries and trend analyses. The model metrics dictionary stores comprehensive performance statistics including MAPE, MAE, RMSE, and R² scores for all evaluated models across the research including Univariate LSTM, Multivariate LSTM, ARIMA, and Random Forest variants, facilitating accurate model comparison responses. The domain knowledge repository comprises structured text containing data source descriptions detailing the origins and collection methodology for weather, supply, and fuel data, explanations of price influencing factors including weather patterns, supply dynamics, fuel costs, and demand fluctuations, as well as documented seasonal patterns and market behaviors observed throughout the research period.

\subsection{Query Processing Pipeline}

The agent processes user queries through a systematic four-step pipeline designed to transform natural language questions into accurate, contextually relevant responses. In the query parsing phase, the system analyzes the incoming natural language question to extract temporal references using regex pattern matching for dates in YYYY-MM-DD format, identifies keywords that trigger specific retrieval pathways such as "model", "LSTM", "ARIMA", "price", or "forecast", and classifies the query into one of several categories including date-specific price inquiries, analytical explanations seeking "why" or "how" answers, model comparison questions, methodological inquiries about data sources or techniques, or general conversational queries.

During context construction, the system assembles relevant information tailored to the identified query type. For date-specific queries requesting prices on particular days, the system retrieves exact prediction records containing actual and predicted prices along with error metrics. For analytical queries seeking explanations of price movements or market phenomena, the system extracts date range data and computes statistical summaries including mean prices, volatility measures, and percentage changes while also retrieving domain knowledge about factors that influence the analyzed period such as weather events, supply disruptions, or demand patterns. Model comparison queries trigger the loading of performance metrics across all evaluated models to enable quantitative comparisons, while methodological queries incorporate data source documentation and feature descriptions from the research methodology.

The prompt engineering phase constructs a structured prompt that combines the retrieved context with the user's original question and explicit instructions to the language model. These instructions enforce strict guidelines including answering only from the provided context without speculation, citing specific numbers and dates when available, explicitly acknowledging when requested information is unavailable rather than generating plausible-sounding fabrications, maintaining concise responses focused on answering the specific question, and synthesizing retrieved information into coherent explanations rather than simply listing facts. This careful prompt design minimizes the language model's tendency toward hallucination while maximizing the usefulness and reliability of generated responses.

Response generation occurs through the Groq API processing the engineered prompt using the Llama 3.3 70B Versatile model with carefully tuned parameters. The maximum token limit of 1,024 provides sufficient capacity for comprehensive answers while preventing excessively verbose responses. The temperature parameter of 0.7 balances creativity in phrasing with factual accuracy, while the top\_p nucleus sampling parameter of 0.9 ensures response quality by limiting consideration to high-probability tokens. The system implements streaming responses to provide real-time feedback as the answer generates, enhancing user experience for longer explanations. Each response concludes with an automatically appended footer noting the model used, the data basis for the answer, and token consumption statistics, providing transparency about the system's operation.

\subsection{Query Capabilities and Response Types}

The AI agent demonstrates two primary capabilities that address the practical needs of agricultural stakeholders. For date-specific price queries, users can ask questions such as "What was the carrot price on 2024-06-15?" and receive precise responses including both the actual observed price from market data and the model's predicted price for that date. The system reports the exact values, such as an actual price of Rs. 285.00 and predicted price of Rs. 278.50, along with the prediction error of Rs. 6.50 representing 2.28\% MAPE. Importantly, the response includes contextual information such as whether the date fell on a weekend or coincided with high demand periods, providing stakeholders with not just the numerical prediction but also the market context that influenced actual prices.

The second major capability involves answering analytical "why" questions that explain price movements and market phenomena. When users pose questions like "Why did prices increase between April 2-8?", the system retrieves the relevant seven-day data range and performs statistical analysis to quantify the change, such as identifying a 36\% price increase over the period. The agent then synthesizes explanations by combining quantitative metrics with qualitative reasoning drawn from the domain knowledge repository. For the April example, the system might identify contributing factors including reduced supply from central highland growing regions due to adverse weather, increased transportation costs from rising fuel prices, and surge in demand preceding a major holiday period. This explanatory capability transforms the AI agent from a simple price lookup tool into an intelligent advisory system that helps stakeholders understand market dynamics, enabling more informed decision-making for planting schedules, inventory management, and pricing strategies.

The implementation utilizes Python 3.8+ with the Groq Python SDK for API access, Gradio 4.x for the web interface, Pandas for data manipulation, NumPy for numerical operations, and regex for pattern matching. The system operates within Groq API's free tier limits of 30 requests per minute and 14,400 requests per day, which proves adequate for research demonstration while providing scalability pathways for production deployment. Predictions data totaling approximately 500KB is loaded at initialization with in-memory caching for optimal performance, while comprehensive error handling manages API failures including rate limiting, network errors, and invalid responses through graceful degradation and user-friendly error messages.

\section{Summary}

This comprehensive methodology integrates traditional statistical methods (ARIMA), state-of-the-art deep learning (LSTM with simple and bidirectional architectures), and ensemble techniques (Random Forest with baseline and hyperparameter-tuned variants) within a rigorous experimental framework. The systematic approach encompasses data collection from authoritative sources, comprehensive preprocessing ensuring quality, model-appropriate feature engineering (273 features for Random Forest, 163 features for LSTM), rigorous selection through distinct pipelines (four-stage for Random Forest yielding 22 features, two-stage for LSTM yielding 8 features), systematic model comparison across diverse approaches, and practical deployment through intelligent AI agent.

The methodology's strength lies in its thoroughness and adaptability: model-specific feature engineering reflects understanding that tree-based and recurrent architectures have structurally different data requirements, separate selection pipelines ensure each model receives optimally configured inputs, multiple architecture variants within each model family provide robustness against configuration-specific limitations, comprehensive evaluation framework using complementary metrics enables reliable performance assessment, and practical deployment through AI agent demonstrates real-world applicability. This end-to-end framework delivers not merely academic analysis but actionable forecasting capabilities for Sri Lankan agricultural stakeholders, while establishing replicable methodology applicable to other vegetables and markets.
