{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg0CjN23xJUt"
      },
      "source": [
        "üìã Cell 1: Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIQvlvlFxKNI",
        "outputId": "aa13ca3b-50d3-4766-e7d9-006ec4463970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Packages installed!\n",
            "Using Groq API (FREE) with Llama 3.1 70B model\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q groq gradio pandas numpy scikit-learn\n",
        "\n",
        "print(\"‚úÖ Packages installed!\")\n",
        "print(\"Using Groq API (FREE) with Llama 3.1 70B model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH_xXvgPxaQ-"
      },
      "source": [
        "üìã Cell 2: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiRlxXNwxa-P",
        "outputId": "889b6af4-9e7a-4bfb-b64f-3fde0bc1c362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "‚úÖ Groq API Client Initialized!\n",
            "Model: Llama 3.1 70B (FREE)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "\n",
        "\n",
        "# Initialize Groq client\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Groq API Client Initialized!\")\n",
        "print(\"Model: Llama 3.1 70B (FREE)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcLN1BjcxrFk"
      },
      "source": [
        "üìã Cell 3: Load Your LSTM Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLyuNe-exfmU",
        "outputId": "3230f78d-e999-49c5-da80-06a05615166b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üìä LOADING PREDICTION DATA\n",
            "============================================================\n",
            "‚ö†Ô∏è CSV file not found. Creating sample data for testing...\n",
            "‚úÖ Created 180 sample predictions\n",
            "üìå Remember to upload your actual LSTM predictions CSV!\n",
            "\n",
            "Sample data preview:\n",
            "        date  actual_price  predicted_price  error        mape\n",
            "0 2024-01-01           222              241     19    8.558559\n",
            "1 2024-01-02           299              331     32   10.702341\n",
            "2 2024-01-03           212              338    126   59.433962\n",
            "3 2024-01-04           134              260    126   94.029851\n",
            "4 2024-01-05           226              340    114   50.442478\n",
            "5 2024-01-06           191              346    155   81.151832\n",
            "6 2024-01-07           308              252    -56   18.181818\n",
            "7 2024-01-08           140              280    140  100.000000\n",
            "8 2024-01-09           222              138    -84   37.837838\n",
            "9 2024-01-10           241              145    -96   39.834025\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Load your LSTM predictions\n",
        "print(\"=\"*60)\n",
        "print(\"üìä LOADING PREDICTION DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Option 1: If you have CSV file uploaded\n",
        "try:\n",
        "    predictions_df = pd.read_csv('lstm_predictions.csv')\n",
        "    # Make sure date column is datetime\n",
        "    predictions_df['date'] = pd.to_datetime(predictions_df['date'])\n",
        "    print(f\"‚úÖ Loaded {len(predictions_df)} predictions from CSV\")\n",
        "    print(f\"Date range: {predictions_df['date'].min()} to {predictions_df['date'].max()}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(predictions_df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è CSV file not found. Creating sample data for testing...\")\n",
        "\n",
        "    # Sample data for testing (REPLACE with your actual data later)\n",
        "    dates = pd.date_range('2024-01-01', periods=180, freq='D')\n",
        "    np.random.seed(42)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'actual_price': np.random.randint(120, 350, 180),\n",
        "        'predicted_price': np.random.randint(110, 360, 180),\n",
        "    })\n",
        "\n",
        "    # Calculate error\n",
        "    predictions_df['error'] = predictions_df['predicted_price'] - predictions_df['actual_price']\n",
        "    predictions_df['mape'] = np.abs(predictions_df['error'] / predictions_df['actual_price']) * 100\n",
        "\n",
        "    print(f\"‚úÖ Created {len(predictions_df)} sample predictions\")\n",
        "    print(\"üìå Remember to upload your actual LSTM predictions CSV!\")\n",
        "    print(\"\\nSample data preview:\")\n",
        "    print(predictions_df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2oDx9qcx6Nt"
      },
      "source": [
        "üìã Cell 4: Agent Core Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5_MTV0lxuXs",
        "outputId": "82eae5d0-773a-45f6-d0dd-053ee26babfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "‚úÖ AGENT INITIALIZED AND READY!\n",
            "============================================================\n",
            "Predictions loaded: 180 days\n",
            "Models available: 4\n",
            "Agent ready to answer questions!\n"
          ]
        }
      ],
      "source": [
        "class CarrotPriceAgent:\n",
        "    \"\"\"AI Agent for Carrot Price Predictions using Groq API\"\"\"\n",
        "\n",
        "    def __init__(self, groq_client, predictions_df):\n",
        "        self.groq = groq_client\n",
        "        self.predictions = predictions_df\n",
        "\n",
        "        # Model comparison results - UPDATE WITH YOUR ACTUAL RESULTS\n",
        "        self.model_results = {\n",
        "            'Univariate LSTM': {\n",
        "                'MAPE': 21.90,\n",
        "                'MAE': 87.95,\n",
        "                'RMSE': 136.82,\n",
        "                'R2': 0.6428\n",
        "            },\n",
        "            'Multivariate LSTM': {\n",
        "                'MAPE': 25.88,  # Update when you improve this\n",
        "                'MAE': 101.19,\n",
        "                'RMSE': 155.19,\n",
        "                'R2': 0.5400\n",
        "            },\n",
        "            'ARIMA': {\n",
        "                'MAPE': 28.5,  # Add your actual results\n",
        "                'MAE': 95.2,\n",
        "                'RMSE': 145.3,\n",
        "                'R2': 0.55\n",
        "            },\n",
        "            'Random Forest': {\n",
        "                'MAPE': 30.2,  # Add your actual results\n",
        "                'MAE': 105.5,\n",
        "                'RMSE': 160.8,\n",
        "                'R2': 0.48\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Data sources info (copy from your Needle PDFs)\n",
        "        self.data_sources = \"\"\"\n",
        "date,actual_price,predicted_price,error,mape\n",
        "2024-04-02,250.00,245.00,-5.00,2.00\n",
        "2024-04-03,265.00,260.00,-5.00,1.89\n",
        "2024-04-04,280.00,275.00,-5.00,1.79\n",
        "2024-04-05,295.00,290.00,-5.00,1.69\n",
        "2024-04-06,310.00,305.00,-5.00,1.61\n",
        "2024-04-07,325.00,320.00,-5.00,1.54\n",
        "2024-04-08,340.00,335.00,-5.00,1.47\n",
        "\"\"\"\n",
        "\n",
        "    def extract_dates_from_query(self, question):\n",
        "        \"\"\"Extract dates from natural language question\"\"\"\n",
        "        # Pattern 1: YYYY-MM-DD format\n",
        "        dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', question)\n",
        "        if dates:\n",
        "            return dates\n",
        "\n",
        "        # Pattern 2: \"on April 15\" or \"in June\"\n",
        "        # Could add more sophisticated date parsing here\n",
        "\n",
        "        return []\n",
        "\n",
        "    def get_price_for_date(self, date_str):\n",
        "        \"\"\"Get prediction for specific date\"\"\"\n",
        "        try:\n",
        "            target_date = pd.to_datetime(date_str)\n",
        "            row = self.predictions[self.predictions['date'] == target_date]\n",
        "\n",
        "            if len(row) == 0:\n",
        "                return None\n",
        "\n",
        "            return {\n",
        "                'date': date_str,\n",
        "                'actual': float(row['actual_price'].iloc[0]),\n",
        "                'predicted': float(row['predicted_price'].iloc[0]),\n",
        "                'error': float(row['error'].iloc[0]),\n",
        "                'mape': float(row.get('mape', [0]).iloc[0]) if 'mape' in row.columns else None\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting price for {date_str}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_date_range_data(self, start_date, end_date):\n",
        "        \"\"\"Get predictions for date range\"\"\"\n",
        "        try:\n",
        "            start = pd.to_datetime(start_date)\n",
        "            end = pd.to_datetime(end_date)\n",
        "\n",
        "            mask = (self.predictions['date'] >= start) & (self.predictions['date'] <= end)\n",
        "            filtered = self.predictions[mask]\n",
        "\n",
        "            if len(filtered) == 0:\n",
        "                return None\n",
        "\n",
        "            return {\n",
        "                'count': len(filtered),\n",
        "                'avg_actual': filtered['actual_price'].mean(),\n",
        "                'avg_predicted': filtered['predicted_price'].mean(),\n",
        "                'avg_error': filtered['error'].mean(),\n",
        "                'price_change': filtered['actual_price'].iloc[-1] - filtered['actual_price'].iloc[0],\n",
        "                'volatility': filtered['actual_price'].std()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting range data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def build_context(self, question):\n",
        "        \"\"\"Build relevant context for the LLM\"\"\"\n",
        "        context = \"You are an AI assistant for a carrot price prediction research project.\\n\\n\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Add data sources for research questions\n",
        "        if any(word in question_lower for word in ['data', 'source', 'where', 'research', 'collect', 'methodology', 'how']):\n",
        "            context += self.data_sources + \"\\n\\n\"\n",
        "\n",
        "        # Add model comparison for model questions\n",
        "        if any(word in question_lower for word in ['model', 'arima', 'lstm', 'random forest', 'compare', 'better', 'best', 'performance', 'accuracy']):\n",
        "            context += \"MODEL PERFORMANCE COMPARISON:\\n\\n\"\n",
        "            for model, metrics in sorted(self.model_results.items(), key=lambda x: x[1]['MAPE']):\n",
        "                context += f\"{model}:\\n\"\n",
        "                context += f\"  - Test MAPE: {metrics['MAPE']:.2f}%\\n\"\n",
        "                context += f\"  - Test MAE: Rs. {metrics['MAE']:.2f}\\n\"\n",
        "                context += f\"  - Test RMSE: Rs. {metrics['RMSE']:.2f}\\n\"\n",
        "                context += f\"  - R¬≤ Score: {metrics['R2']:.4f}\\n\\n\"\n",
        "\n",
        "            best_model = min(self.model_results.items(), key=lambda x: x[1]['MAPE'])\n",
        "            context += f\"Best Performing Model: {best_model[0]} (MAPE: {best_model[1]['MAPE']:.2f}%)\\n\\n\"\n",
        "\n",
        "        # Add price data for prediction questions\n",
        "        if any(word in question_lower for word in ['price', 'predict', 'forecast', 'cost', 'value', '2024', '2025']):\n",
        "            dates = self.extract_dates_from_query(question)\n",
        "\n",
        "            if dates:\n",
        "                # Specific dates mentioned\n",
        "                for date in dates[:3]:  # Max 3 dates\n",
        "                    price_data = self.get_price_for_date(date)\n",
        "                    if price_data:\n",
        "                        context += f\"PRICE DATA FOR {date}:\\n\"\n",
        "                        context += f\"  - Actual Price: Rs. {price_data['actual']:.2f}\\n\"\n",
        "                        context += f\"  - LSTM Predicted: Rs. {price_data['predicted']:.2f}\\n\"\n",
        "                        context += f\"  - Prediction Error: Rs. {price_data['error']:.2f}\\n\"\n",
        "                        if price_data['mape']:\n",
        "                            context += f\"  - Prediction Accuracy: {100 - price_data['mape']:.2f}%\\n\"\n",
        "                        context += \"\\n\"\n",
        "            else:\n",
        "                # No specific date, show recent trends\n",
        "                recent = self.predictions.tail(7)\n",
        "                context += \"RECENT PRICE TRENDS (Last 7 days):\\n\"\n",
        "                for _, row in recent.iterrows():\n",
        "                    context += f\"  {row['date'].strftime('%Y-%m-%d')}: Actual=Rs.{row['actual_price']:.0f}, Predicted=Rs.{row['predicted_price']:.0f}\\n\"\n",
        "                context += \"\\n\"\n",
        "\n",
        "        # Add analytical context for \"why\" questions\n",
        "        if any(word in question_lower for word in ['why', 'reason', 'cause', 'explain', 'increase', 'decrease', 'spike', 'drop']):\n",
        "            context += \"\"\"\n",
        "FACTORS AFFECTING CARROT PRICES:\n",
        "1. Weather: Heavy rainfall in Nuwara Eliya region reduces supply\n",
        "2. Supply: Seasonal variations from different growing regions\n",
        "3. Fuel prices: Transportation costs impact final market prices\n",
        "4. Demand: Weekend/holiday demand patterns, festival seasons\n",
        "5. Market status: Trading activity levels, market closure days\n",
        "6. Agricultural cycles: Planting and harvesting seasons\n",
        "\n",
        "Price typically INCREASES when:\n",
        "- Heavy rainfall disrupts supply\n",
        "- High fuel prices increase transportation costs\n",
        "- High demand periods (festivals, weekends)\n",
        "- Supply shortages from growing regions\n",
        "\n",
        "Price typically DECREASES when:\n",
        "- Good weather leads to abundant harvest\n",
        "- Low fuel prices\n",
        "- Low demand periods\n",
        "- Oversupply from multiple regions\n",
        "\"\"\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def ask_groq(self, question):\n",
        "        \"\"\"Main query function using Groq API\"\"\"\n",
        "        try:\n",
        "            # Build context\n",
        "            context = self.build_context(question)\n",
        "\n",
        "            # Create prompt\n",
        "            full_prompt = f\"\"\"{context}\n",
        "\n",
        "USER QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Answer based ONLY on the context provided above\n",
        "- Be specific and cite numbers, dates, and metrics when available\n",
        "- If asked about data sources, mention the specific sources listed\n",
        "- If comparing models, use the exact performance metrics provided\n",
        "- For price predictions, reference the actual data points given\n",
        "- Keep answers clear, concise, and informative\n",
        "- Use bullet points or structure when helpful\n",
        "- If you don't have enough information in the context, say so clearly\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "            # Call Groq API\n",
        "            response = self.groq.chat.completions.create(\n",
        "                model=\"llama-3.3-70b-versatile\",  # NEW\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a helpful AI assistant for a carrot price prediction research project. Provide accurate, data-driven answers based on the context given.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": full_prompt\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=1024,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            # Extract answer\n",
        "            answer = response.choices[0].message.content\n",
        "\n",
        "            # Add footer\n",
        "            tokens_used = response.usage.total_tokens\n",
        "            answer += f\"\\n\\n---\\n*Powered by Llama 3.1 70B (via Groq) | {len(self.predictions)} days of LSTM predictions | {tokens_used} tokens used*\"\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå Error: {str(e)}\\n\\n\"\n",
        "\n",
        "            if \"rate_limit\" in str(e).lower():\n",
        "                error_msg += \"‚è±Ô∏è Rate limit reached. Please wait a moment and try again.\"\n",
        "            elif \"invalid\" in str(e).lower() and \"key\" in str(e).lower():\n",
        "                error_msg += \"üîë API key issue. Please check your Groq API key.\"\n",
        "            else:\n",
        "                error_msg += \"Please check your internet connection and try again.\"\n",
        "\n",
        "            return error_msg\n",
        "\n",
        "# Initialize the agent\n",
        "agent = CarrotPriceAgent(groq_client, predictions_df)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ AGENT INITIALIZED AND READY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Predictions loaded: {len(predictions_df)} days\")\n",
        "print(f\"Models available: {len(agent.model_results)}\")\n",
        "print(\"Agent ready to answer questions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhH-3-abyLuz"
      },
      "source": [
        "üìã Cell 5- test API connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrxe7vnPx9Ew",
        "outputId": "afdac227-66ca-478f-9d5e-7200e1d332e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üîç TESTING GROQ API CONNECTION\n",
            "============================================================\n",
            "‚úÖ API Connection Successful!\n",
            "Response: Hello! API is working!\n",
            "Model: llama-3.3-70b-versatile\n",
            "Tokens used: 50\n",
            "\n",
            "üéâ Ready to create Gradio interface!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üîç TESTING GROQ API CONNECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Simple test\n",
        "    test_response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",  # NEW - Better & Faster!\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Say 'Hello! API is working!'\"}],\n",
        "        max_tokens=50\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ API Connection Successful!\")\n",
        "    print(f\"Response: {test_response.choices[0].message.content}\")\n",
        "    print(f\"Model: {test_response.model}\")\n",
        "    print(f\"Tokens used: {test_response.usage.total_tokens}\")\n",
        "    print(\"\\nüéâ Ready to create Gradio interface!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå API Test Failed: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. API key is correct\")\n",
        "    print(\"2. Internet connection is working\")\n",
        "    print(\"3. Get new key at: https://console.groq.com/keys\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpooL1O3JSXT"
      },
      "source": [
        "Cell 6 - gradio interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "XCMN1bk4JPq1",
        "outputId": "31e8d383-6b46-4061-b3f3-6f1915b7faac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4261779394.py:34: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=500)\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üöÄ LAUNCHING GRADIO INTERFACE\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0daf223c2f39a45082.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0daf223c2f39a45082.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def chat_function(message, history):\n",
        "    \"\"\"Process user message\"\"\"\n",
        "    try:\n",
        "        response = agent.ask_groq(message)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\\n\\nPlease try rephrasing your question.\"\n",
        "\n",
        "# Create Gradio Chat Interface\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chat_function,\n",
        "    title=\"ü•ï Carrot Price Prediction AI Agent\",\n",
        "    description=\"\"\"\n",
        "    **Powered by Llama 3.3 70B (FREE via Groq API)**\n",
        "\n",
        "    **Ask me about:**\n",
        "    - üìÖ Specific date prices: *\"What was the price on 2024-06-15?\"*\n",
        "    - üìä Price trends: *\"Why did prices spike in April?\"*\n",
        "    - üèÜ Model comparisons: *\"Which model performed best?\"*\n",
        "    - üìö Data sources: *\"Where did you get the weather data?\"*\n",
        "    - üîÆ Predictions: *\"What's the predicted price trend?\"*\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        \"What was the carrot price on 2024-06-15?\",\n",
        "        \"Why did prices increase between April 2-8, 2024?\",\n",
        "        \"Which model has the best MAPE score?\",\n",
        "        \"Where did you collect the data from?\",\n",
        "        \"Compare LSTM and ARIMA models\",\n",
        "        \"What factors affect carrot prices?\",\n",
        "        \"Explain your research methodology\"\n",
        "    ],\n",
        "    theme=gr.themes.Soft(),\n",
        "    cache_examples=False,\n",
        "    chatbot=gr.Chatbot(height=500)\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ LAUNCHING GRADIO INTERFACE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Launch with public shareable link\n",
        "interface.launch(\n",
        "    share=True,  # Creates public link\n",
        "    debug=True,\n",
        "    show_error=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Interface launched!\")\n",
        "print(\"üì± Use the public link above to share with others\")\n",
        "print(\"‚è±Ô∏è Link expires in 72 hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zdOwkSHFJZts"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
